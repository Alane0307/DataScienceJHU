<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Chang Liu" />
  <meta name="dcterms.date" content="2024-03-14" />
  <title>Human Activity Prediction with R</title>
  <style>
    html {
      line-height: 1.5;
      font-family: Georgia, serif;
      font-size: 20px;
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 1em;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, 'Lucida Console', Consolas, monospace;
      font-size: 85%;
      margin: 0;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      background-color: #1a1a1a;
      border: none;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
    .display.math{display: block; text-align: center; margin: 0.5rem auto;}
  </style>
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->
</head>
<body>
<header id="title-block-header">
<h1 class="title">Human Activity Prediction with R</h1>
<p class="author">Chang Liu</p>
<p class="date">March 14, 2024</p>
</header>
<p><code>{r global-options, include=FALSE} knitr::opts_chunk$set(     fig.width = 8,     fig.height = 6,     fig.path = 'Figs/',     echo = TRUE,     warning = FALSE,     message = FALSE ) options(knitr.table.format = "html")</code></p>
<h2 id="background">Background</h2>
<p>Using devices such as <em>Jawbone Up</em>, <em>Nike FuelBand</em>,
and <em>Fitbit</em> it is now possible to collect a large amount of data
about personal activity relatively inexpensively. These type of devices
are part of the quantified self movement – a group of enthusiasts who
take measurements about themselves regularly to improve their health, to
find patterns in their behavior, or because they are tech geeks.</p>
<p>One thing that people regularly do is quantify how <em>much</em> of a
particular activity they do, but they rarely quantify <em>how well they
do it</em>. In this project, your goal will be to use data from
accelerometers on the belt, forearm, arm, and dumbell of 6 participants.
They were asked to perform barbell lifts correctly and incorrectly in 5
different ways. More information is available from the website <a
href="http://web.archive.org/web/20161224072740/http:/groupware.les.inf.puc-rio.br/har">here</a>:
(see the section on the Weight Lifting Exercise Dataset).</p>
<h3 id="data">Data</h3>
<ul>
<li><p>The training data for this project are available <a
href="https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv">here</a></p></li>
<li><p>The test data are available <a
href="https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv">here</a></p></li>
<li><p>The data for this project come from this <a
href="http://web.archive.org/web/20161224072740/http:/groupware.les.inf.puc-rio.br/har">source</a></p></li>
</ul>
<h3 id="goal">Goal</h3>
<p>The goal of your project is to predict the manner in which they did
the exercise. This is the “classe” variable in the training set. You may
use any of the other variables to predict with. You should create a
report describing how you built your model, how you used cross
validation, what you think the expected out of sample error is, and why
you made the choices you did. You will also use your prediction model to
predict 20 different test cases</p>
<h2 id="executive-summary">Executive summary</h2>
<p>Using a random forest classifier with a k-fold cross validation of 7,
the optimal model has an <strong>accuracy of 0.993</strong> and an OOB
rate of <strong>0.66%</strong>. The variable importance plot shows that
the roll_belt variable was most important in predicting the
<code>classe</code> variable.</p>
<p>Applying our model on the test set, we attain a similar
<strong>accuracy of 0.993</strong>. Applying the model on the 20 test
case in our validation set, we achieve 100% accuracy in predicting the
right <code>classe</code> variable.</p>
<hr />
<h2 id="loading-packages">Loading Packages</h2>
<p><code>{r packages} pacman::p_load(data.table, caret, parallel, doParallel, purrr, visdat, dplyr, printr, kableExtra, corrplot, e1071, randomForest)</code></p>
<h3 id="loading-data">Loading Data</h3>
<p>```{r data} # training url_train &lt;-
“https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv”</p>
<p>training &lt;- fread(url_train, na.strings = c(“#DIV/0”, ““,”NA”),
stringsAsFactors = TRUE)</p>
<h1 id="testing-data">testing data</h1>
<p>url_test &lt;-
“https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv”</p>
<p>testing &lt;- fread(url_test, na.strings = c(“#DIV/0”, ““,”NA”),
stringsAsFactors = TRUE)</p>
<pre><code>
## Data Preprocessing

```{r glimpse}
# glimpse(training)</code></pre>
<p>Looking at our data (output is too large), we see there’s a total of
160 variables that we have to build our model. Most of these variables
are not useful for building our prediction model, especially the first 7
columns, which are just rowno., usernames, timestamps, etc.</p>
<p><code>{r subset} training &lt;- training[, -c(1:7)] testing &lt;- testing[, -c(1:7)]</code></p>
<p><code>{r dim1} rbind(training = dim(training),       testing = dim(testing)) %&gt;%       kbl() %&gt;%       kable_classic(full_width = F, html_font = "Cambria")</code></p>
<p>Next to reduce the unnecessary variables, we set a threshold for the
amount of NAs a variable has in our data. I’m going to set the threshold
as 70% and use the discard function from the purrr package to discard
the variables. (Another way to do this is by using
<code>nearZeroVar</code> function which finds variables with near zero
variability or with PCA)</p>
<p>```{r re-na} # function to remove columns with NAs na_remove_col
&lt;- function(data, threshold) { data %&gt;% discard(~ sum(is.na(.x)) /
length(.x) * 100 &gt; threshold) }</p>
<p>clean_train &lt;- na_remove_col(training, 70)</p>
<p>clean_test &lt;- na_remove_col(testing, 70)</p>
<p>rbind(training = dim(clean_train), testing = dim(clean_test)) %&gt;%
kbl() %&gt;% kable_classic(full_width = F, html_font = “Cambria”)</p>
<pre><code>
Now we see that exactly 100 variables were removed after the threshold.

## Data Partition

The data we have is a training data set and a validation data set. The standard procedure is to partition our training set into train and test set, and then apply our final model to our validation set. The function `createDataPartition` will be used to split our data.

```{r data-partition}
set.seed(2021) # for reproducability

inTrain &lt;- createDataPartition(clean_train$classe, p=0.7, list=FALSE)

train &lt;- clean_train[inTrain, ]
test &lt;- clean_train[-inTrain, ]</code></pre>
<p>Now we have our training and test data which is 70% and 30% of our
initial training data respectively.</p>
<h2 id="exploratory-data-analysis">Exploratory Data Analysis</h2>
<p>By doing a bit of EDA on our training data, we can observe whether
there are variables which are highly correlated using the
<code>corrplot</code> library.</p>
<p><code>{r corrplot} corr_data &lt;- select_if(train, is.numeric) corrplot(     cor(corr_data),     method = "color",     tl.pos = "n",     insig = "blank" )</code></p>
<p>Our correlation plot shows that the most of our variables are not
very correlated. With the exception of the first few columns at the
upper left, and columns at the middle. Correlated variables can bring
about issues when we use it for building models such as Random forest,
which is the model I will be using for this prediction.</p>
<h2 id="prediction">Prediction</h2>
<h3 id="random-forest-model">Random forest Model</h3>
<p>To predict the <code>classe</code> variable in our data, which is a
factor variable, what we need is a classifier model. I’m going to use a
random forest model because it’s a flexible and easy to use ensemble
learning algorithm that provides high accuracy predictions through
cross-validation.</p>
<h3 id="setting-parallel-processing">Setting Parallel Processing</h3>
<p>That said, building random forest models can be computationally
expensive, so we’ll be setting registering for parallel processing with
the <code>parallel</code> and <code>doParallel</code> packages.</p>
<p><code>{r parallel, eval=FALSE} cluster &lt;- makeCluster(detectCores() - 1)  registerDoParallel(cluster)</code></p>
<h3 id="building-the-model">Building the model</h3>
<p>As said before, random forest uses cross-validation to randomly split
the fitted training set into train and test sets based on the given
k-folds (k), in this case 7, in the <code>trainControl</code> function.
This means our model will be trained 7 times based on the
cross-validated data. We also set <code>allowParallel</code> as True to
allow for parallel processing.</p>
<p>Using Caret, model training can be done with the <code>train</code>
function, and our method is “rf” which stands for random forest, and we
preProcess with PCA.</p>
<p>```{r rfmodel, eval=FALSE} set.seed(2021)</p>
<p>fitControl &lt;- trainControl(method = “cv”, number = 7,
allowParallel = TRUE)</p>
<p>rf.fit &lt;- train( classe ~ ., method = “rf”, data = train,
trControl = fitControl )</p>
<h1 id="stop-cluster">stop cluster</h1>
<p>stopCluster(cluster) registerDoSEQ()</p>
<h1 id="save-model-into-an-rds-file-to-save-time">save model into an rds
file to save time</h1>
<p>saveRDS(rf.fit,file=“rfmodel.rds”)</p>
<pre><code>
After training the model, we stop the clusters, and then save the model into an rds file to save time. We can then load it later and perform a downstream analysis.

Now we measure our model performance with statistics like kappa and accuracy, along with some plots.

### Model Performance

```{r model-output}
model.rf &lt;- readRDS(file = &quot;rfmodel.rds&quot;)
model.rf</code></pre>
<p>From the results, we see that the optimal model, has an
<strong>accuracy of 0.99</strong></p>
<p><code>{r final-model} model.rf$finalModel</code></p>
<p>The OOB is our out of sample rate, which is <strong>0.66%</strong>.
This means our accuracy is considered high and acceptable for our
prediction.</p>
<p>Below you see the plot for the error of each <code>classe</code>
prediction as the no of trees increase, and we see that as we reach
around 150 trees, the OOB becomes flat, and we can use 150 as the
<code>ntrees</code> for our <code>trcontrol</code> if we decide to
further fine-tune our model.</p>
<p><code>{r oob-plot} plot(model.rf$finalModel)</code></p>
<h3 id="variable-importance">Variable Importance</h3>
<p><code>{r varimp-plot, fig.height=6, fig.width=8} importance &lt;- varImp(model.rf, scale = FALSE) plot(importance, top=10)</code></p>
<p><code>VarImp</code> function by R tells us that from our model, the
most important feature in predicting the classe variable is
<code>roll_belt</code> .</p>
<h3 id="prediction-on-test-set">Prediction on test set</h3>
<p>Using our trained model, we can apply it to our test set, and observe
the accuracy.</p>
<p><code>{r pred-test} pred.rf &lt;- predict(model.rf, test) confM &lt;- confusionMatrix(test$classe, pred.rf) confM$table %&gt;%   kbl() %&gt;%   kable_paper("hover", full_width = F)</code></p>
<p><code>{r pred-test-acc} confM$overall["Accuracy"]</code></p>
<p>We obtain an <strong>accuracy of 0.99</strong>, which means only
around 1% of <code>classe</code> variables were falsely classified.</p>
<h2 id="final-prediction-on-validation">Final Prediction on
Validation</h2>
<p>Finally we apply our model to the 20 test cases given in the
validation data.</p>
<p><code>{r pred-final} final.pred.rf &lt;- predict(model.rf, clean_test) summary(final.pred.rf)</code></p>
<p><code>{r pred-final2} final.pred.rf</code></p>
<h2 id="session-info">Session info</h2>
<p><code>{r sessionInfo} sessionInfo()</code></p>
<h2 id="citation">Citation</h2>
<p><a
href="http://groupware.les.inf.puc-rio.br/collaborator.jsf?p1=ugulino">Ugulino,
W.</a>; <a
href="http://groupware.les.inf.puc-rio.br/collaborator.jsf?p1=debora">Cardador,
D.</a>; <a
href="http://groupware.les.inf.puc-rio.br/collaborator.jsf?p1=katia">Vega,
K.</a>; <a
href="http://groupware.les.inf.puc-rio.br/collaborator.jsf?p1=evelloso">Velloso,
E.</a>; Milidiu, R.; <a
href="http://groupware.les.inf.puc-rio.br/collaborator.jsf?p1=hugo">Fuks,
H.</a> <a href="http://groupware.les.inf.puc-rio.br/work.jsf?p1=10335"
title="Wearable Computing: Accelerometers&#39; Data Classification of Body Postures and Movements"><strong>Wearable
Computing: Accelerometers’ Data Classification of Body Postures and
Movements</strong></a>. Proceedings of 21st Brazilian Symposium on
Artificial Intelligence. Advances in Artificial Intelligence - SBIA
2012. In: Lecture Notes in Computer Science. , pp. 52-61. Curitiba, PR:
Springer Berlin / Heidelberg, 2012. ISBN 978-3-642-34458-9. DOI:
10.1007/978-3-642-34459-6_6.</p>
</body>
</html>
